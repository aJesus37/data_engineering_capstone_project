{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17d732e-702e-43b0-9d0d-8237da67a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c0262-aa6f-4355-8054-cd85a3476a66",
   "metadata": {},
   "source": [
    "## Network Logs ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1be552-6364-4d95-b38e-40a118b7a87d",
   "metadata": {},
   "source": [
    "Reads data from JSON files and transform field that is JSON into structure to be able to analyze it with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1cc4ef-f4b6-465e-8155-06dc94a42fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_network_logs_transient = spark.read.json('filebeat/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6257611f-b796-4d34-be89-34d482484f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/15 19:33:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "json_schema = spark.read.json(df_network_logs_transient.rdd.map(lambda row: row.message)).schema\n",
    "df_network_logs_final = df_network_logs_transient.withColumn('message_json', f.from_json(f.col('message'), json_schema)).drop(f.col('message'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30a008d-5588-43e9-9341-4a51a74dbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints schema of the final product\n",
    "#df_network_logs_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe41ec-af4a-472c-a4e2-95fabc877bf6",
   "metadata": {},
   "source": [
    "Writes logs into datalake in Parquet format (more performant, smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161d7bb7-f15e-4305-ae1e-ccd953f0332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_network_logs_final.write\\\n",
    ".partitionBy('dt','hr')\\\n",
    ".parquet('datalake/network_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada3224-74c8-423e-9580-6bcb3d0ed9aa",
   "metadata": {},
   "source": [
    "Data quality check (checks if the resulting datalake has rows. If not, raise error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61646800-cac2-4eec-82b2-cd0e4c6b72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Data Quality Check Passed for Network Logs, row_count: 1000560 [+]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('datalake/network_logs')\n",
    "row_count = df.count()\n",
    "if row_count < 1:\n",
    "    raise(\"Data Quality Failed, number of rows lesser than 1\")\n",
    "else:\n",
    "    print(f\"[+] Data Quality Check Passed for Network Logs, row_count: {row_count} [+]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899f30f-f32e-469b-9afc-3ee863a2b4c0",
   "metadata": {},
   "source": [
    "Cleans dataframes from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd70692c-99ca-4330-8608-c8123b6a79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/dev/null', 'w')\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = f\n",
    "\n",
    "df_network_logs_transient.unpersist()\n",
    "df_network_logs_final.unpersist()\n",
    "df.unpersist()\n",
    "\n",
    "sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca7386-9a80-4f07-9619-bebbecbe39cb",
   "metadata": {},
   "source": [
    "# Host logs ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfaae90-66d5-40c4-b54b-3fff263f6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_host_logs = spark.read.json('auditbeat/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4147f416-da4b-4c95-bb3d-13a2703218a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prints schema of the final product\n",
    "#df_host_logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce7199-cb47-4f27-8a25-744fa224c27b",
   "metadata": {},
   "source": [
    "Writes logs into datalake in Parquet format (more performant, smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bd621d-5643-465e-b12a-e5487e6bc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_host_logs.write\\\n",
    ".partitionBy('dt','hr')\\\n",
    ".parquet('datalake/host_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86867b51-0afb-4fb9-a56b-5b6568fba09d",
   "metadata": {},
   "source": [
    "Data quality check (checks if the resulting datalake has rows. If not, raise error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45e0655-b409-4f44-9b42-8390a82130d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Data Quality Check Passed for Host Logs, row_count: 84856 [+]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('datalake/host_logs')\n",
    "row_count = df.count()\n",
    "if row_count < 1:\n",
    "    raise(\"Data Quality Failed, number of rows lesser than 1\")\n",
    "else:\n",
    "    print(f\"[+] Data Quality Check Passed for Host Logs, row_count: {row_count} [+]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae774c-0af2-4415-9e4c-3ce538525d2a",
   "metadata": {},
   "source": [
    "Cleans dataframes from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd288e9-2b57-4886-9e41-39c00f002778",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/dev/null', 'w')\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = f\n",
    "\n",
    "df_host_logs.unpersist()\n",
    "df.unpersist()\n",
    "\n",
    "sys.stdout = old_stdout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
